% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/bijector-layers.R
\name{layer_autoregressive}
\alias{layer_autoregressive}
\title{A Masked Autoencoder for Distribution Estimation}
\usage{
layer_autoregressive(object, params, event_shape = NULL,
  hidden_units = NULL, input_order = c("left-to-right",
  "right-to-left", "random"), hidden_degrees = c("equal", "random"),
  activation = NULL, use_bias = TRUE,
  kernel_initializer = "glorot_uniform", validate_args = FALSE,
  batch_input_shape = NULL, input_shape = NULL, batch_size = NULL,
  dtype = NULL, name = NULL, trainable = NULL, weights = NULL)
}
\arguments{
\item{object}{Model or layer object}

\item{params}{Integer specifying the number of parameters to output per input.}

\item{event_shape}{List of positive integers (or a single int),
specifying the shape of the input to this layer, which is also the
event_shape of the distribution parameterized by this layer.  Currently
only rank-1 shapes are supported.  That is, event_shape must be a single
integer.  If not specified, the event shape is inferred when this layer
is first called or built.}

\item{hidden_units}{List of non-negative integers, specifying the number of
units in each hidden layer.}

\item{input_order}{Order of degrees to the input units: 'random',
left-to-right', 'right-to-left', or an array of an explicit order. For
example, 'left-to-right' builds an autoregressive model:
\code{p(x) = p(x1) p(x2 | x1) ... p(xD | x<D)}.  Default: 'left-to-right'.}

\item{hidden_degrees}{Method for assigning degrees to the hidden units:
equal', 'random'.  If 'equal', hidden units in each layer are allocated
equally (up to a remainder term) to each degree.  Default: 'equal'.}

\item{activation}{Name of activation function to use. If you don't specify
anything, no activation is applied (ie. "linear" activation: a(x) = x).}

\item{use_bias}{Whether the layer uses a bias vector.}

\item{kernel_initializer}{Initializer for the \code{kernel} weights matrix.}

\item{validate_args}{Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.}

\item{batch_input_shape}{Shapes, including the batch size. For instance,
\code{batch_input_shape=c(10, 32)} indicates that the expected input will be
batches of 10 32-dimensional vectors. \code{batch_input_shape=list(NULL, 32)}
indicates batches of an arbitrary number of 32-dimensional vectors.}

\item{input_shape}{Dimensionality of the input (integer) not including the
samples axis. This argument is required when using this layer as the first
layer in a model.}

\item{batch_size}{Fixed batch size for layer}

\item{dtype}{The data type expected by the input, as a string (\code{float32},
\code{float64}, \code{int32}...)}

\item{name}{An optional name string for the layer. Should be unique in a
model (do not reuse the same name twice). It will be autogenerated if it
isn't provided.}

\item{trainable}{Whether the layer weights will be updated during training.}

\item{weights}{Initial weights for layer.}
}
\description{
A \code{AutoregressiveLayer} takes as input a Tensor of shape \code{[..., event_size]}
and returns a Tensor of shape \code{[..., event_size, params]}.
The output satisfies the autoregressive property.  That is, the layer is
configured with some permutation \code{ord} of \code{{0, ..., event_size-1}} (i.e., an
ordering of the input dimensions), and the output \code{output[batch_idx, i, ...]}
for input dimension \code{i} depends only on inputs \code{x[batch_idx, j]} where
\code{ord(j) < ord(i)}.  The autoregressive property allows us to use
\code{output[batch_idx, i]} to parameterize conditional distributions:
\code{p(x[batch_idx, i] | x[batch_idx, ] for ord(j) < ord(i))}
which give us a tractable distribution over input \code{x[batch_idx]}:
\code{p(x[batch_idx]) = prod_i p(x[batch_idx, ord(i)] | x[batch_idx, ord(0:i)])}
}
